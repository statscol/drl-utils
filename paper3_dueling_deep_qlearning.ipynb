{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459895cb-7c46-418a-9825-639bd3288efa",
   "metadata": {},
   "source": [
    "# Implementing Dueling Deep and Double Deep Q-Learning\n",
    "See paper details in [Arxiv](https://arxiv.org/abs/1511.06581).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84843526-f9f4-49f1-8f48-4d0b887c1462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from memory_utils import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c555b2e-c9cf-43cf-9892-39cde1eda447",
   "metadata": {},
   "source": [
    "## Dueling Deep Q-learning \n",
    "\n",
    "Same experience replay, memory buffer as seen in normal Deep Q-Learning\n",
    "\n",
    "![img](./images/dueling_qlearning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4937459-7b94-438d-85ee-b526844a4fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DuelingDeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self,lr,n_actions,input_dims,name,chkpt_dir):\n",
    "        super(DuelingDeepQNetwork,self).__init__()\n",
    "        self.checkpoint_dir=chkpt_dir\n",
    "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name)\n",
    "        self.cnn1=torch.nn.Conv2d(input_dims[0],32,8,stride=4)\n",
    "        self.cnn2=torch.nn.Conv2d(32,64,4,stride=2)\n",
    "        self.cnn3=torch.nn.Conv2d(64,64,3,stride=1)\n",
    "        fc_input_dims=self.calculate_conv_output_dims(input_dims)\n",
    "        print(f\"CNN outputs after flattened: {fc_input_dims}\")\n",
    "        self.fc1=torch.nn.Linear(fc_input_dims,512)\n",
    "        self.V=torch.nn.Linear(512,1)\n",
    "        self.A=torch.nn.Linear(512,n_actions)\n",
    "        self.optimizer=optim.RMSprop(self.parameters(),lr=lr)\n",
    "        self.criterion=torch.nn.MSELoss()\n",
    "\n",
    "    def calculate_conv_output_dims(self,input_dims):\n",
    "        state=torch.zeros(1,*input_dims)\n",
    "        dims=self.cnn1(state)\n",
    "        dims=self.cnn2(dims)\n",
    "        dims=self.cnn3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self,state):\n",
    "        out=F.relu(self.cnn1(state))\n",
    "        out=F.relu(self.cnn2(out))\n",
    "        out=F.relu(self.cnn3(out)) \n",
    "#        out=out.view(out.size()[0],-1)\n",
    "        out=torch.flatten(out,1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        V=self.V(out)\n",
    "        A=self.A(out)\n",
    "        #actions=self.fc2(out)\n",
    "        return V,A\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\"...saving checkpoint...\")\n",
    "        torch.save(self.state_dict(),self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print(\"...loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class DuelingDQNAgent():\n",
    "    def __init__(self,gamma,epsilon,lr,n_actions,input_dims,mem_size,batch_size,eps_min=0.01,eps_dec=5e-7,replace=1000,algo=None,env_name=None,chkpt_dir='tmp/dqn'):\n",
    "        self.lr=lr\n",
    "        self.input_dims=input_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.gamma=gamma\n",
    "        self.batch_size=batch_size\n",
    "        self.epsilon=epsilon\n",
    "        self.eps_dec=eps_dec\n",
    "        self.eps_min=eps_min\n",
    "        self.replace_target_cnt=replace\n",
    "        self.env_name=env_name\n",
    "        self.algo=algo\n",
    "        self.chkpt_dir=chkpt_dir\n",
    "        self.action_space=[i for i in range(self.n_actions)]\n",
    "        self.learn_step_counter=0\n",
    "        self.q_eval=DuelingDeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_eval\",chkpt_dir=self.chkpt_dir)\n",
    "        self.q_next=DuelingDeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_next\",chkpt_dir=self.chkpt_dir)\n",
    "        self.memory=ReplayBuffer(mem_size,input_dims,n_actions)\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state=torch.tensor([observation],dtype=torch.float).to(DEVICE)\n",
    "            _,advantage=self.q_eval(state)\n",
    "            actions=self.q_eval(state)\n",
    "            action=torch.argmax(advantage).item()\n",
    "        else:\n",
    "            action=np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        self.memory.store_transition(state,action,reward,state_,done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state,action, reward,new_state,done= self.memory.sample_buffer(self.batch_size)\n",
    "        states=torch.tensor(state).to(DEVICE)\n",
    "        rewards=torch.tensor(reward).to(DEVICE)\n",
    "        dones=torch.tensor(done).to(DEVICE)\n",
    "        actions=torch.tensor(action).to(DEVICE)\n",
    "        states_=torch.tensor(new_state).to(DEVICE)\n",
    "        return states,actions,rewards,states_,dones\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt==0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon=self.epsilon-self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "    def learn(self,state,action,reward,new_state):\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states,actions,rewards,states_,dones=self.sample_memory()\n",
    "        #indices=np.arange(self.batch_size)\n",
    "        \n",
    "        V,A=self.q_eval(states)\n",
    "        V_s,A_s=self.q_next(states_)\n",
    "        \n",
    "        q_pred=torch.add(V,(A[:,actions]-A.mean(axis=1,keepdim=True)))[:,actions]\n",
    "        q_next=torch.add(V_s,(A_s[:,actions]-A_s.mean(axis=1,keepdim=True))).max(dim=1)[0]\n",
    "        \n",
    "#        max_actions=torch.argmax(q_eval,dim=1)\n",
    "        \n",
    "        q_next[dones]=0.0 ## to account for games where a terminal state was reached\n",
    "        q_target=rewards+ self.gamma*q_next\n",
    "\n",
    "        loss= self.q_eval.criterion(q_target,q_pred).to(DEVICE)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        self.decrease_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca0d7e0-b1e5-4e2d-988b-3be299fe3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "from utils import make_env\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env=make_env(\"PongNoFrameskip-v4\")\n",
    "best_score=-np.inf\n",
    "load_checkpoint=False\n",
    "n_games=100\n",
    "CHECK_DIR=\"./models/\"\n",
    "\n",
    "if not os.path.isdir(CHECK_DIR):\n",
    "    os.makedirs(CHECK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25490d-aed1-4fba-9138-ad5f1d7143b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN outputs after flattened: 3136\n",
      "CNN outputs after flattened: 3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-62bc9cfa2859>:124: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  q_next[dones]=0.0 ## to account for games where a terminal state was reached\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...saving checkpoint...\n",
      "...saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "## Learn Dueling DQN LEARNING\n",
    "\n",
    "agent=DuelingDQNAgent(gamma=0.99,epsilon=1.0,lr=1e-04,input_dims=(env.observation_space.shape),n_actions=env.action_space.n,\n",
    "                mem_size=20000,eps_min=0.1,batch_size=32,replace=1000,eps_dec=1e-05,chkpt_dir=CHECK_DIR,algo=\"DuelingDQNAgent\",env_name=\"PongNoFrameskip-v4\")\n",
    "\n",
    "agent.q_eval.to(DEVICE)\n",
    "agent.q_next.to(DEVICE)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "scores,eps_history,steps_array=[],[],[]\n",
    "n_steps=0\n",
    "\n",
    "for i in range(n_games):\n",
    "    done=False\n",
    "    score=0\n",
    "    observation=env.reset()\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        action=agent.choose_action(observation)\n",
    "        observation_,reward,done,info=env.step(action)\n",
    "        score+=reward\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.store_transition(observation,action,reward,observation_,int(done))\n",
    "            agent.learn(observation,action,reward,observation_)\n",
    "        observation=observation_\n",
    "        n_steps+=1\n",
    "    scores.append(score)\n",
    "    steps_array.append(n_steps)\n",
    "    avg_score=np.mean(scores[-100:])\n",
    "    if (i+1) % 10==0:\n",
    "        print(f'Episode {i+1} score: {score:.1f} | avg score {avg_score:.1f} | epsilon {agent.epsilon:.3f}')\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score=avg_score\n",
    "\n",
    "    eps_history.append(agent.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d6f0f4-76c0-4620-a326-3dc3fc4004c3",
   "metadata": {},
   "source": [
    "## Dueling Double Deep Q- Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f2241-760e-408c-869f-969765ae821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DuelingDoubleDeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self,lr,n_actions,input_dims,name,chkpt_dir):\n",
    "        super(DuelingDoubleDeepQNetwork,self).__init__()\n",
    "        self.checkpoint_dir=chkpt_dir\n",
    "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name)\n",
    "        self.cnn1=torch.nn.Conv2d(input_dims[0],32,8,stride=4)\n",
    "        self.cnn2=torch.nn.Conv2d(32,64,4,stride=2)\n",
    "        self.cnn3=torch.nn.Conv2d(64,64,3,stride=1)\n",
    "        fc_input_dims=self.calculate_conv_output_dims(input_dims)\n",
    "        print(f\"CNN outputs after flattened: {fc_input_dims}\")\n",
    "        self.fc1=torch.nn.Linear(fc_input_dims,512)\n",
    "        self.V=torch.nn.Linear(512,1)\n",
    "        self.A=torch.nn.Linear(512,n_actions)\n",
    "        self.optimizer=optim.RMSprop(self.parameters(),lr=lr)\n",
    "        self.criterion=torch.nn.MSELoss()\n",
    "\n",
    "    def calculate_conv_output_dims(self,input_dims):\n",
    "        state=torch.zeros(1,*input_dims)\n",
    "        dims=self.cnn1(state)\n",
    "        dims=self.cnn2(dims)\n",
    "        dims=self.cnn3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self,state):\n",
    "        out=F.relu(self.cnn1(state))\n",
    "        out=F.relu(self.cnn2(out))\n",
    "        out=F.relu(self.cnn3(out)) \n",
    "#        out=out.view(out.size()[0],-1)\n",
    "        out=torch.flatten(out,1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        V=self.V(out)\n",
    "        A=self.A(out)\n",
    "        #actions=self.fc2(out)\n",
    "        return V,A\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\"...saving checkpoint...\")\n",
    "        torch.save(self.state_dict(),self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print(\"...loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class DuelingDDQNAgent():\n",
    "    def __init__(self,gamma,epsilon,lr,n_actions,input_dims,mem_size,batch_size,eps_min=0.01,eps_dec=5e-7,replace=1000,algo=None,env_name=None,chkpt_dir='tmp/dqn'):\n",
    "        self.lr=lr\n",
    "        self.input_dims=input_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.gamma=gamma\n",
    "        self.batch_size=batch_size\n",
    "        self.epsilon=epsilon\n",
    "        self.eps_dec=eps_dec\n",
    "        self.eps_min=eps_min\n",
    "        self.replace_target_cnt=replace\n",
    "        self.env_name=env_name\n",
    "        self.algo=algo\n",
    "        self.chkpt_dir=chkpt_dir\n",
    "        self.action_space=[i for i in range(self.n_actions)]\n",
    "        self.learn_step_counter=0\n",
    "        self.q_eval=DuelingDoubleDeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_eval\",chkpt_dir=self.chkpt_dir)\n",
    "        self.q_next=DuelingDoubleDeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_next\",chkpt_dir=self.chkpt_dir)\n",
    "        self.memory=ReplayBuffer(mem_size,input_dims,n_actions)\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state=torch.tensor([observation],dtype=torch.float).to(DEVICE)\n",
    "            _,advantage=self.q_eval(state)\n",
    "            actions=self.q_eval(state)\n",
    "            action=torch.argmax(advantage).item()\n",
    "        else:\n",
    "            action=np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        self.memory.store_transition(state,action,reward,state_,done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state,action, reward,new_state,done= self.memory.sample_buffer(self.batch_size)\n",
    "        states=torch.tensor(state).to(DEVICE)\n",
    "        rewards=torch.tensor(reward).to(DEVICE)\n",
    "        dones=torch.tensor(done).to(DEVICE)\n",
    "        actions=torch.tensor(action).to(DEVICE)\n",
    "        states_=torch.tensor(new_state).to(DEVICE)\n",
    "        return states,actions,rewards,states_,dones\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt==0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon=self.epsilon-self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "    def learn(self,state,action,reward,new_state):\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states,actions,rewards,states_,dones=self.sample_memory()\n",
    "        #indices=np.arange(self.batch_size)\n",
    "        \n",
    "        V,A=self.q_eval(states)\n",
    "        V_s,A_s=self.q_next(states_)\n",
    "        \n",
    "        q_pred=torch.add(V,(A[:,actions]-A.mean(axis=0,keepdim=True)))[:,actions]\n",
    "        q_next=torch.add(V_s,(A_s[:,actions]-A_s.mean(axis=0,keepdim=True)))\n",
    "        \n",
    "        \n",
    "        V_p,A_p=self.q_eval(states_)\n",
    "        q_eval=torch.add(V_p+(A_p[:,actions]-A_p.mean(axis=0,keepdim=True)))\n",
    "        \n",
    "        \n",
    "        max_actions=torch.argmax(q_eval,dim=1)\n",
    "        \n",
    "        q_next[dones]=0.0 ## to account for games where a terminal state was reached\n",
    "        q_target=rewards+ self.gamma*q_next[:,max_actions]\n",
    "\n",
    "        loss= self.q_eval.criterion(q_target,q_pred).to(DEVICE)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        self.decrease_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122fff5b-34be-415b-b8d1-5ff808344618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e9af9-8641-4f58-9562-cbe9e463f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn\n",
    "\n",
    "agent=DuelingDDQNAgent(gamma=0.99,epsilon=1.0,lr=1e-04,input_dims=(env.observation_space.shape),n_actions=env.action_space.n,\n",
    "                mem_size=20000,eps_min=0.1,batch_size=32,replace=1000,eps_dec=1e-05,chkpt_dir=CHECK_DIR,algo=\"DuelingDDQNAgent\",env_name=\"PongNoFrameskip-v4\")\n",
    "\n",
    "agent.q_eval.to(DEVICE)\n",
    "agent.q_next.to(DEVICE)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "scores,eps_history,steps_array=[],[],[]\n",
    "n_steps=0\n",
    "\n",
    "for i in range(n_games):\n",
    "    done=False\n",
    "    score=0\n",
    "    observation=env.reset()\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        action=agent.choose_action(observation)\n",
    "        observation_,reward,done,info=env.step(action)\n",
    "        score+=reward\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.store_transition(observation,action,reward,observation_,int(done))\n",
    "            agent.learn(observation,action,reward,observation_)\n",
    "        observation=observation_\n",
    "        n_steps+=1\n",
    "    scores.append(score)\n",
    "    steps_array.append(n_steps)\n",
    "    avg_score=np.mean(scores[-100:])\n",
    "    if (i+1) % 10==0:\n",
    "        print(f'Episode {i+1} score: {score:.1f} | avg score {avg_score:.1f} | epsilon {agent.epsilon:.3f}')\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score=avg_score\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
