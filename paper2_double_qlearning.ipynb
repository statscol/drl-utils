{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f3eea0-d71c-4a88-a6cc-9b9aac834c40",
   "metadata": {},
   "source": [
    "# Implementing Double Deep Q-Learning\n",
    "See paper details in [Arxiv](https://arxiv.org/pdf/1509.06461.pdf).\n",
    "\n",
    "Now we must modify the learn method to match the agent Q-updates proposed in the paper.\n",
    "\n",
    "![img](https://miro.medium.com/max/1400/1*o8PMTWmT1XK1jdSK59QrYQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78905dd1-bcee-45fc-aefb-0db32ba28644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from memory_utils import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d844099e-14ba-404d-bc6e-734719ed270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self,lr,n_actions,input_dims,name,chkpt_dir):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.checkpoint_dir=chkpt_dir\n",
    "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name)\n",
    "        self.cnn1=torch.nn.Conv2d(input_dims[0],32,8,stride=4)\n",
    "        self.cnn2=torch.nn.Conv2d(32,64,4,stride=2)\n",
    "        self.cnn3=torch.nn.Conv2d(64,64,3,stride=1)\n",
    "        fc_input_dims=self.calculate_conv_output_dims(input_dims)\n",
    "        print(f\"CNN outputs after flattened: {fc_input_dims}\")\n",
    "        self.fc1=torch.nn.Linear(fc_input_dims,512)\n",
    "        self.fc2=torch.nn.Linear(512,n_actions)\n",
    "        self.optimizer=optim.RMSprop(self.parameters(),lr=lr)\n",
    "        self.criterion=torch.nn.MSELoss()\n",
    "\n",
    "    def calculate_conv_output_dims(self,input_dims):\n",
    "        state=torch.zeros(1,*input_dims)\n",
    "        dims=self.cnn1(state)\n",
    "        dims=self.cnn2(dims)\n",
    "        dims=self.cnn3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self,state):\n",
    "        out=F.relu(self.cnn1(state))\n",
    "        out=F.relu(self.cnn2(out))\n",
    "        out=F.relu(self.cnn3(out)) \n",
    "#        out=out.view(out.size()[0],-1)\n",
    "        out=torch.flatten(out,1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        actions=self.fc2(out)\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\"...saving checkpoint...\")\n",
    "        torch.save(self.state_dict(),self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print(\"...loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class DDQNAgent():\n",
    "    def __init__(self,gamma,epsilon,lr,n_actions,input_dims,mem_size,batch_size,eps_min=0.01,eps_dec=5e-7,replace=1000,algo=None,env_name=None,chkpt_dir='tmp/dqn'):\n",
    "        self.lr=lr\n",
    "        self.input_dims=input_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.gamma=gamma\n",
    "        self.batch_size=batch_size\n",
    "        self.epsilon=epsilon\n",
    "        self.eps_dec=eps_dec\n",
    "        self.eps_min=eps_min\n",
    "        self.replace_target_cnt=replace\n",
    "        self.env_name=env_name\n",
    "        self.algo=algo\n",
    "        self.chkpt_dir=chkpt_dir\n",
    "        self.action_space=[i for i in range(self.n_actions)]\n",
    "        self.learn_step_counter=0\n",
    "        self.q_eval=DeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_eval\",chkpt_dir=self.chkpt_dir)\n",
    "        self.q_next=DeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_next\",chkpt_dir=self.chkpt_dir)\n",
    "        self.memory=ReplayBuffer(mem_size,input_dims,n_actions)\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state=torch.tensor([observation],dtype=torch.float).to(DEVICE)\n",
    "            actions=self.q_eval(state)\n",
    "            action=torch.argmax(actions).item()\n",
    "        else:\n",
    "            action=np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        self.memory.store_transition(state,action,reward,state_,done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state,action, reward,new_state,done= self.memory.sample_buffer(self.batch_size)\n",
    "        states=torch.tensor(state).to(DEVICE)\n",
    "        rewards=torch.tensor(reward).to(DEVICE)\n",
    "        dones=torch.tensor(done).to(DEVICE)\n",
    "        actions=torch.tensor(action).to(DEVICE)\n",
    "        states_=torch.tensor(new_state).to(DEVICE)\n",
    "        return states,actions,rewards,states_,dones\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt==0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon=self.epsilon-self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "    def learn(self,state,action,reward,new_state):\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states,actions,rewards,states_,dones=self.sample_memory()\n",
    "        #indices=np.arange(self.batch_size)\n",
    "\n",
    "        q_pred=self.q_eval(states)[:,actions]\n",
    "        q_next=self.q_next(states_)\n",
    "        q_eval=self.q_eval(states_)\n",
    "        \n",
    "        max_actions=torch.argmax(q_eval,dim=1)\n",
    "        \n",
    "        q_next[dones]=0.0 ## to account for games where a terminal state was reached\n",
    "        q_target=rewards+ self.gamma*q_next[:,max_actions]\n",
    "\n",
    "        loss= self.q_eval.criterion(q_target,q_pred).to(DEVICE)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        self.decrease_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8747ed2-07e0-4824-836b-5078ece50cd7",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f221a64-3b8b-435a-868c-1cdd98eb51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_env\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env=make_env(\"PongNoFrameskip-v4\")\n",
    "best_score=-np.inf\n",
    "load_checkpoint=False\n",
    "n_games=100\n",
    "CHECK_DIR=\"./models/\"\n",
    "\n",
    "if not os.path.isdir(CHECK_DIR):\n",
    "    os.makedirs(CHECK_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4d229-8cfe-45d7-b070-cd9a50a37303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN outputs after flattened: 3136\n",
      "CNN outputs after flattened: 3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-52a36f25a70b>:118: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  q_next[dones]=0.0 ## to account for games where a terminal state was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 10 score: -21.0 | avg score -19.9 | epsilon 0.906\n"
     ]
    }
   ],
   "source": [
    "## Learn\n",
    "\n",
    "agent=DDQNAgent(gamma=0.99,epsilon=1.0,lr=1e-04,input_dims=(env.observation_space.shape),n_actions=env.action_space.n,\n",
    "                mem_size=20000,eps_min=0.1,batch_size=32,replace=1000,eps_dec=1e-05,chkpt_dir=CHECK_DIR,algo=\"DDQNAgent\",env_name=\"PongNoFrameskip-v4\")\n",
    "\n",
    "agent.q_eval.to(DEVICE)\n",
    "agent.q_next.to(DEVICE)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "scores,eps_history,steps_array=[],[],[]\n",
    "n_steps=0\n",
    "\n",
    "for i in range(n_games):\n",
    "    done=False\n",
    "    score=0\n",
    "    observation=env.reset()\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        action=agent.choose_action(observation)\n",
    "        observation_,reward,done,info=env.step(action)\n",
    "        score+=reward\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.store_transition(observation,action,reward,observation_,int(done))\n",
    "            agent.learn(observation,action,reward,observation_)\n",
    "        observation=observation_\n",
    "        n_steps+=1\n",
    "    scores.append(score)\n",
    "    steps_array.append(n_steps)\n",
    "    avg_score=np.mean(scores[-100:])\n",
    "    if (i+1) % 10==0:\n",
    "        print(f'Episode {i+1} score: {score:.1f} | avg score {avg_score:.1f} | epsilon {agent.epsilon:.3f}')\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score=avg_score\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
