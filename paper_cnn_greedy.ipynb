{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym[atari]==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Paper Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,max_size,input_shape,n_actions):\n",
    "        self.mem_size=max_size\n",
    "        self.mem_cntr=0\n",
    "        self.state_memory=np.zeros((self.mem_size,*input_shape),dtype=np.float32)\n",
    "        self.new_state_memory=np.zeros((self.mem_size,*input_shape),dtype=np.float32)\n",
    "        self.action_memory=np.zeros(self.mem_size,dtype=np.int64)\n",
    "        self.reward_memory=np.zeros(self.mem_size,dtype=np.float32)\n",
    "        self.terminal_memory=np.zeros(self.mem_size,dtype=np.uint8)\n",
    "    \n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        index=self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index]=state\n",
    "        self.action_memory[index]=action\n",
    "        self.reward_memory[index]=reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index]=done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self,batch_size):\n",
    "        max_mem=min(self.mem_cntrl,self.mem_size) ## to account for number of instances stored\n",
    "        batch=np.random.choice(max_mem,batch_size,replace=False)\n",
    "        states=self.state_memory[batch]\n",
    "        actions=self.action_memory[batch]\n",
    "        rewards=self.reward_memory[batch]\n",
    "        states_=self.new_state_memory[batch]\n",
    "        dones= self.terminal_memory[batch]\n",
    "        return states,actions,rewards,states_,dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Q-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self,lr,n_actions,name,input_dims,chkpt_dir):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.checkpoint_dir=chkpt_dir\n",
    "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name)\n",
    "        self.cnn1=torch.nn.Conv2d(input_dims[0],32,8,stride=4)\n",
    "        self.cnn2=torch.nn.Conv2d(32,64,4,stride=2)\n",
    "        self.cnn3=torch.nn.Conv2d(64,64,3,stride=1)\n",
    "        fc_input_dims=self.calculate_conv_output_dims(input_dims)\n",
    "        self.fc1=torch.nn.Linear(fc_input_dims,512)\n",
    "        self.fc2=torch.nn.Linear(512,n_actions)\n",
    "        self.optimizer=optim.RMSprop(self.parameters(),lr=lr)\n",
    "        self.criterion=torch.nn.MSELoss()\n",
    "\n",
    "    def calculate_conv_output_dims(self,input_dims):\n",
    "        state=torch.zeros(1,*input_dims)\n",
    "        dims=self.cnn1(state)\n",
    "        dims=self.cnn2(dims)\n",
    "        dims=self.cnn3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self,state):\n",
    "        out=F.relu(self.cnn1(state))\n",
    "        out=F.relu(self.cnn2(out))\n",
    "        out=F.relu(self.cnn3(out))\n",
    "#        out=out.view(out.size()[0],-1)\n",
    "        out=torch.flatten(out,-1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        out=self.fc2(out)\n",
    "        actions=self.fc2()\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\"...saving checkpoint...\")\n",
    "        torch.save(self.state_dict(),self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print(\"...loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self,gamma,epsilon,lr,n_actions,input_dims,mem_size,batch_size,eps_min=0.01,eps_dec=5e-7,replace=1000,algo=None,env_name=None,chkpt_dir='tmp/dqn'):\n",
    "        self.lr=lr\n",
    "        self.input_dims=input_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.gamma=gamma\n",
    "        self.batch_size=batch_size\n",
    "        self.epsilon=epsilon\n",
    "        self.eps_dec=eps_dec\n",
    "        self.eps_min=eps_min\n",
    "        self.replace_target_cnt=replace\n",
    "        self.algo=algo\n",
    "        self.chkpt_dir=chkpt_dir\n",
    "        self.action_space=[i for i in range(self.n_actions)]\n",
    "        self.learn_step_counter=0\n",
    "        self.q_eval=DeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_eval\",chkpt_dir=self.chkpt_dir)\n",
    "        self.q_next=DeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_next\",chkpt_dir=self.chkpt_dir)\n",
    "        self.memory=ReplayBuffer(mem_size,input_dims,n_actions)\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state=torch.tensor([observation],dtype=torch.float).to(DEVICE)\n",
    "            actions=self.q_eval(state)\n",
    "            action=torch.argmax(actions).item()\n",
    "        else:\n",
    "            action=np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        self.memory.store_transition(state,action,reward,state_,done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state,action, reward,new_state,done= self.memory.sample_buffer(self.batch_size)\n",
    "        states=torch.tensor(state).to(DEVICE)\n",
    "        rewards=torch.tensor(reward).to(DEVICE)\n",
    "        dones=torch.tensor(done).to(DEVICE)\n",
    "        actions=torch.tensor(actions).to(DEVICE)\n",
    "        states_=torch.tensor(new_state).to(DEVICE)\n",
    "        return states,actions,rewards,states_,dones\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt==0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon=self.epsilon-self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "        \n",
    "    def learn(self,state,action,reward,new_state):\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states,actions,rewards,states_,dones=self.sample_memory()\n",
    "        indices=np.arange(self.batch_size)\n",
    "\n",
    "        q_pred=self.q_eval(states)[indices,actions]\n",
    "        q_next=self.q_next(states_).max(dim=1)[0]\n",
    "\n",
    "        q_next[dones]=0.0\n",
    "        q_target=rewards+ self.gamma*q_next\n",
    "\n",
    "        loss= self.q_eval.criterion(q_target,q_pred).to(DEVICE)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        self.decrease_epsilon()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import make_env\n",
    "\n",
    "env=make_env(\"PongNoFrameskip-v4\")\n",
    "best_score=-np.inf\n",
    "load_checkpoint=False\n",
    "n_games=500\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a3d0c7bd191f88b44ed83b77c1fdcdbc198b96942ce84f272d0c5fef1029cd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
