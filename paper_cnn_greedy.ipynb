{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym[atari]==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Paper Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self,max_size,input_shape,n_actions):\n",
    "        self.mem_size=max_size\n",
    "        self.mem_cntr=0\n",
    "        self.state_memory=np.zeros((self.mem_size,*input_shape),dtype=np.float32)\n",
    "        self.new_state_memory=np.zeros((self.mem_size,*input_shape),dtype=np.float32)\n",
    "        self.action_memory=np.zeros(self.mem_size,dtype=np.int64)\n",
    "        self.reward_memory=np.zeros(self.mem_size,dtype=np.float32)\n",
    "        self.terminal_memory=np.zeros(self.mem_size,dtype=np.uint8)\n",
    "    \n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        index=self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index]=state\n",
    "        self.action_memory[index]=action\n",
    "        self.reward_memory[index]=reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index]=done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self,batch_size):\n",
    "        max_mem=min(self.mem_cntr,self.mem_size) ## to account for number of instances stored\n",
    "        batch=np.random.choice(max_mem,batch_size,replace=False)\n",
    "        states=self.state_memory[batch]\n",
    "        actions=self.action_memory[batch]\n",
    "        rewards=self.reward_memory[batch]\n",
    "        states_=self.new_state_memory[batch]\n",
    "        dones= self.terminal_memory[batch]\n",
    "        return states,actions,rewards,states_,dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Q-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class DeepQNetwork(torch.nn.Module):\n",
    "    def __init__(self,lr,n_actions,input_dims,name,chkpt_dir):\n",
    "        super(DeepQNetwork,self).__init__()\n",
    "        self.checkpoint_dir=chkpt_dir\n",
    "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name)\n",
    "        self.cnn1=torch.nn.Conv2d(input_dims[0],32,8,stride=4)\n",
    "        self.cnn2=torch.nn.Conv2d(32,64,4,stride=2)\n",
    "        self.cnn3=torch.nn.Conv2d(64,64,3,stride=1)\n",
    "        fc_input_dims=self.calculate_conv_output_dims(input_dims)\n",
    "        print(f\"CNN outputs after flattened: {fc_input_dims}\")\n",
    "        self.fc1=torch.nn.Linear(fc_input_dims,512)\n",
    "        self.fc2=torch.nn.Linear(512,n_actions)\n",
    "        self.optimizer=optim.RMSprop(self.parameters(),lr=lr)\n",
    "        self.criterion=torch.nn.MSELoss()\n",
    "\n",
    "    def calculate_conv_output_dims(self,input_dims):\n",
    "        state=torch.zeros(1,*input_dims)\n",
    "        dims=self.cnn1(state)\n",
    "        dims=self.cnn2(dims)\n",
    "        dims=self.cnn3(dims)\n",
    "        return int(np.prod(dims.size()))\n",
    "\n",
    "    def forward(self,state):\n",
    "        out=F.relu(self.cnn1(state))\n",
    "        out=F.relu(self.cnn2(out))\n",
    "        out=F.relu(self.cnn3(out)) \n",
    "#        out=out.view(out.size()[0],-1)\n",
    "        out=torch.flatten(out,1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        actions=self.fc2(out)\n",
    "        return actions\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print(\"...saving checkpoint...\")\n",
    "        torch.save(self.state_dict(),self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print(\"...loading checkpoint...\")\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n",
    "\n",
    "\n",
    "class DQNAgent():\n",
    "    def __init__(self,gamma,epsilon,lr,n_actions,input_dims,mem_size,batch_size,eps_min=0.01,eps_dec=5e-7,replace=1000,algo=None,env_name=None,chkpt_dir='tmp/dqn'):\n",
    "        self.lr=lr\n",
    "        self.input_dims=input_dims\n",
    "        self.n_actions=n_actions\n",
    "        self.gamma=gamma\n",
    "        self.batch_size=batch_size\n",
    "        self.epsilon=epsilon\n",
    "        self.eps_dec=eps_dec\n",
    "        self.eps_min=eps_min\n",
    "        self.replace_target_cnt=replace\n",
    "        self.env_name=env_name\n",
    "        self.algo=algo\n",
    "        self.chkpt_dir=chkpt_dir\n",
    "        self.action_space=[i for i in range(self.n_actions)]\n",
    "        self.learn_step_counter=0\n",
    "        self.q_eval=DeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_eval\",chkpt_dir=self.chkpt_dir)\n",
    "        self.q_next=DeepQNetwork(self.lr,self.n_actions,self.input_dims,name=f\"{self.env_name}_{self.algo}_q_next\",chkpt_dir=self.chkpt_dir)\n",
    "        self.memory=ReplayBuffer(mem_size,input_dims,n_actions)\n",
    "        \n",
    "    def choose_action(self,observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state=torch.tensor([observation],dtype=torch.float).to(DEVICE)\n",
    "            actions=self.q_eval(state)\n",
    "            action=torch.argmax(actions).item()\n",
    "        else:\n",
    "            action=np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "\n",
    "    def store_transition(self,state,action,reward,state_,done):\n",
    "        self.memory.store_transition(state,action,reward,state_,done)\n",
    "\n",
    "    def sample_memory(self):\n",
    "        state,action, reward,new_state,done= self.memory.sample_buffer(self.batch_size)\n",
    "        states=torch.tensor(state).to(DEVICE)\n",
    "        rewards=torch.tensor(reward).to(DEVICE)\n",
    "        dones=torch.tensor(done).to(DEVICE)\n",
    "        actions=torch.tensor(action).to(DEVICE)\n",
    "        states_=torch.tensor(new_state).to(DEVICE)\n",
    "        return states,actions,rewards,states_,dones\n",
    "    \n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt==0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    def save_models(self):\n",
    "        self.q_eval.save_checkpoint()\n",
    "        self.q_next.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.q_eval.load_checkpoint()\n",
    "        self.q_next.load_checkpoint()\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon=self.epsilon-self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "        \n",
    "        \n",
    "    def learn(self,state,action,reward,new_state):\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad()\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states,actions,rewards,states_,dones=self.sample_memory()\n",
    "        indices=np.arange(self.batch_size)\n",
    "\n",
    "        q_pred=self.q_eval(states)[indices,actions]\n",
    "        q_next=self.q_next(states_).max(dim=1)[0]\n",
    "\n",
    "        q_next[dones]=0.0\n",
    "        q_target=rewards+ self.gamma*q_next\n",
    "\n",
    "        loss= self.q_eval.criterion(q_target,q_pred).to(DEVICE)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "        self.decrease_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    }
   ],
   "source": [
    "from utils import make_env\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "env=make_env(\"PongNoFrameskip-v4\")\n",
    "best_score=-np.inf\n",
    "load_checkpoint=False\n",
    "n_games=500\n",
    "CHECK_DIR=\"./models/\"\n",
    "\n",
    "if not os.path.isdir(CHECK_DIR):\n",
    "    os.makedirs(CHECK_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##playing random\n",
    "\n",
    "import numpy as np\n",
    "def play_pong(n_games,win_ratio_eval=10):\n",
    "    scores=[]\n",
    "    win_ratios=[]\n",
    "    for game in range(n_games):\n",
    "        env.reset()\n",
    "        game_over=False\n",
    "        score=0\n",
    "        while not game_over:\n",
    "            action=env.action_space.sample() ##random action\n",
    "            observation, reward, game_over, info = env.step(action)\n",
    "            score+=reward\n",
    "            \n",
    "        scores.append(score)\n",
    "    \n",
    "        if game%win_ratio_eval==0:\n",
    "            win_ratios.append(np.mean(scores[-win_ratio_eval:]))\n",
    "        \n",
    "    return win_ratios\n",
    "            \n",
    "            \n",
    "    \n",
    "plays=play_pong(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN outputs after flattened: 3136\n",
      "CNN outputs after flattened: 3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-060aeca12086>:121: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
      "  q_next[dones]=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 score: -21.0 | avg score -21.0 | epsilon 0.992\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 1 score: -21.0 | avg score -21.0 | epsilon 0.984\n",
      "Episode 2 score: -20.0 | avg score -20.7 | epsilon 0.974\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 3 score: -21.0 | avg score -20.8 | epsilon 0.965\n",
      "Episode 4 score: -21.0 | avg score -20.8 | epsilon 0.957\n",
      "Episode 5 score: -20.0 | avg score -20.7 | epsilon 0.946\n",
      "Episode 6 score: -21.0 | avg score -20.7 | epsilon 0.939\n",
      "Episode 7 score: -21.0 | avg score -20.8 | epsilon 0.929\n",
      "Episode 8 score: -19.0 | avg score -20.6 | epsilon 0.920\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 9 score: -20.0 | avg score -20.5 | epsilon 0.909\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 10 score: -21.0 | avg score -20.5 | epsilon 0.899\n",
      "Episode 11 score: -20.0 | avg score -20.5 | epsilon 0.890\n",
      "Episode 12 score: -20.0 | avg score -20.5 | epsilon 0.881\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 13 score: -20.0 | avg score -20.4 | epsilon 0.872\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 14 score: -20.0 | avg score -20.4 | epsilon 0.863\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 15 score: -21.0 | avg score -20.4 | epsilon 0.855\n",
      "Episode 16 score: -21.0 | avg score -20.5 | epsilon 0.847\n",
      "Episode 17 score: -20.0 | avg score -20.4 | epsilon 0.837\n",
      "Episode 18 score: -21.0 | avg score -20.5 | epsilon 0.828\n",
      "Episode 19 score: -21.0 | avg score -20.5 | epsilon 0.819\n",
      "Episode 20 score: -20.0 | avg score -20.5 | epsilon 0.810\n",
      "Episode 21 score: -21.0 | avg score -20.5 | epsilon 0.801\n",
      "Episode 22 score: -20.0 | avg score -20.5 | epsilon 0.792\n",
      "Episode 23 score: -20.0 | avg score -20.5 | epsilon 0.783\n",
      "Episode 24 score: -19.0 | avg score -20.4 | epsilon 0.773\n",
      "Episode 25 score: -20.0 | avg score -20.4 | epsilon 0.762\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 26 score: -20.0 | avg score -20.4 | epsilon 0.753\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 27 score: -19.0 | avg score -20.3 | epsilon 0.741\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 28 score: -21.0 | avg score -20.3 | epsilon 0.731\n",
      "Episode 29 score: -18.0 | avg score -20.3 | epsilon 0.719\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 30 score: -19.0 | avg score -20.2 | epsilon 0.708\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 31 score: -20.0 | avg score -20.2 | epsilon 0.696\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 32 score: -21.0 | avg score -20.2 | epsilon 0.686\n",
      "Episode 33 score: -18.0 | avg score -20.2 | epsilon 0.674\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 34 score: -21.0 | avg score -20.2 | epsilon 0.663\n",
      "Episode 35 score: -19.0 | avg score -20.2 | epsilon 0.651\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 36 score: -17.0 | avg score -20.1 | epsilon 0.637\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 37 score: -21.0 | avg score -20.1 | epsilon 0.623\n",
      "Episode 38 score: -17.0 | avg score -20.0 | epsilon 0.609\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 39 score: -19.0 | avg score -20.0 | epsilon 0.597\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 40 score: -17.0 | avg score -19.9 | epsilon 0.581\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 41 score: -20.0 | avg score -19.9 | epsilon 0.570\n",
      "Episode 42 score: -21.0 | avg score -20.0 | epsilon 0.556\n",
      "Episode 43 score: -21.0 | avg score -20.0 | epsilon 0.545\n",
      "Episode 44 score: -20.0 | avg score -20.0 | epsilon 0.531\n",
      "Episode 45 score: -17.0 | avg score -19.9 | epsilon 0.515\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 46 score: -17.0 | avg score -19.9 | epsilon 0.498\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 47 score: -18.0 | avg score -19.8 | epsilon 0.482\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 48 score: -16.0 | avg score -19.7 | epsilon 0.466\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 49 score: -19.0 | avg score -19.7 | epsilon 0.451\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 50 score: -21.0 | avg score -19.7 | epsilon 0.437\n",
      "Episode 51 score: -17.0 | avg score -19.7 | epsilon 0.420\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 52 score: -17.0 | avg score -19.6 | epsilon 0.399\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 53 score: -18.0 | avg score -19.6 | epsilon 0.376\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 54 score: -17.0 | avg score -19.6 | epsilon 0.358\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 55 score: -18.0 | avg score -19.5 | epsilon 0.340\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 56 score: -18.0 | avg score -19.5 | epsilon 0.324\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 57 score: -18.0 | avg score -19.5 | epsilon 0.304\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 58 score: -18.0 | avg score -19.5 | epsilon 0.288\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 59 score: -19.0 | avg score -19.4 | epsilon 0.272\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 60 score: -14.0 | avg score -19.4 | epsilon 0.247\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 61 score: -13.0 | avg score -19.3 | epsilon 0.225\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 62 score: -14.0 | avg score -19.2 | epsilon 0.201\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 63 score: -12.0 | avg score -19.1 | epsilon 0.178\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 64 score: -14.0 | avg score -19.0 | epsilon 0.154\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 65 score: -13.0 | avg score -18.9 | epsilon 0.129\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 66 score: -14.0 | avg score -18.8 | epsilon 0.104\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 67 score: -14.0 | avg score -18.8 | epsilon 0.100\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 68 score: -14.0 | avg score -18.7 | epsilon 0.100\n",
      "...saving checkpoint...\n",
      "...saving checkpoint...\n",
      "Episode 69 score: -19.0 | avg score -18.7 | epsilon 0.100\n"
     ]
    }
   ],
   "source": [
    "## Learn\n",
    "\n",
    "agent=DQNAgent(gamma=0.99,epsilon=1.0,lr=1e-04,input_dims=(env.observation_space.shape),n_actions=env.action_space.n,\n",
    "                mem_size=20000,eps_min=0.1,batch_size=32,replace=1000,eps_dec=1e-05,chkpt_dir=CHECK_DIR,algo=\"DQNAgent\",env_name=\"PongNoFrameskip-v4\")\n",
    "\n",
    "agent.q_eval.to(DEVICE)\n",
    "agent.q_next.to(DEVICE)\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    \n",
    "scores,eps_history,steps_array=[],[],[]\n",
    "n_steps=0\n",
    "\n",
    "for i in range(n_games):\n",
    "    done=False\n",
    "    score=0\n",
    "    observation=env.reset()\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "        action=agent.choose_action(observation)\n",
    "        observation_,reward,done,info=env.step(action)\n",
    "        score+=reward\n",
    "        \n",
    "        if not load_checkpoint:\n",
    "            agent.store_transition(observation,action,reward,observation_,int(done))\n",
    "            agent.learn(observation,action,reward,observation_)\n",
    "        observation=observation_\n",
    "        n_steps+=1\n",
    "    scores.append(score)\n",
    "    steps_array.append(n_steps)\n",
    "    avg_score=np.mean(scores[-100:])\n",
    "    print(f'Episode {i} score: {score:.1f} | avg score {avg_score:.1f} | epsilon {agent.epsilon:.3f}')\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "        best_score=avg_score\n",
    "\n",
    "    eps_history.append(agent.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g=plt.plot(scores)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a3d0c7bd191f88b44ed83b77c1fdcdbc198b96942ce84f272d0c5fef1029cd0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
